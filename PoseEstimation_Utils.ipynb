{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09151d3e-9215-4d79-8350-395831925dea",
   "metadata": {},
   "source": [
    "# PoseEstimation_Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837c548-5a53-487e-b24e-02c1116727d3",
   "metadata": {},
   "source": [
    "Importo tutte le librerie/moduli utili.\n",
    "\n",
    "Il modulo sys permette di fare in modo di avere la sottocartella \"models\" (interna a \"SuperGluePretrainedNetwork) allo stesso livello del codice, così da richiamare le funzioni al suo interno (superpoint, superglue, utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0b18c5-2b99-4fc7-8385-9493843109d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./SuperGluePretrainedNetwork/models')\n",
    "from utils import VideoStreamer, frame2tensor\n",
    "\n",
    "from superpoint import SuperPoint\n",
    "\n",
    "from superglue import SuperGlue\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49b079-1ea7-4f18-b0ee-635a09dbd953",
   "metadata": {},
   "source": [
    "Definizione della funzione che serve alla fine per ottenere gli angoli di eulero (misurati in gradi) a partire dalla matrice di rotazione ottenuta da recoverPose().\n",
    "\n",
    "La funzione è stata presa da https://learnopencv.com/rotation-matrix-to-euler-angles/ ed è relativa ad una rotazione X,Y,Z adattando l'analoga funzione matlab \"rotm2euler.m\" (dove la rotazione risulta essere invece Z,Y,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75206036-1a9f-4e60-8080-25a1a7069b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RotMatToEulAng(R):\n",
    "    sy = np.sqrt(R[0, 0] ** 2 + R[1, 0] ** 2)\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if not singular:\n",
    "        x = np.arctan2(R[2, 1], R[2, 2])  \n",
    "        y = np.arctan2(-R[2, 0], sy)     \n",
    "        z = np.arctan2(R[1, 0], R[0, 0])\n",
    "\n",
    "    else:\n",
    "        x = np.arctan2(-R[1, 2], R[1, 1])\n",
    "        y = np.arctan2(-R[2, 0], sy)\n",
    "        z = 0\n",
    "\n",
    "    euler_angles_rad = np.array([x, y, z])\n",
    "    euler_angles_deg = np.degrees(euler_angles_rad)\n",
    "\n",
    "    return euler_angles_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e7502-b6b1-4b6b-a966-fe2ae693d60c",
   "metadata": {},
   "source": [
    "Riga di debug per verificare che il path sia corretto: se restituisce \"True\" allora ha riconosciuto il percorso in modo corretto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2f322d-1491-4f90-9d2f-dba428b615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.path.exists('C:/Users/antoa/ImageClassification/SuperGluePretrainedNetwork/MoonLu/MoonImg_Only2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99fdcf8-d94d-4ebb-953f-00094ad43098",
   "metadata": {},
   "source": [
    "Inizializzo l'oggetto con la classe VideoStreamer, dove do in input:\n",
    "* la cartella con le sole due immagini da processare (dalla repo si capisce che le immagini possono essere di più, ma ho semplificato il codice per verificarne il corretto funzionamento)\n",
    "* resize in un formato adatto delle immagini prese\n",
    "* skip=2 permette di considerare tutte le immagini almeno una volta, senza saltarne nessuna\n",
    "* image_glob per intendere il formato da prendere in considerazione\n",
    "\n",
    "N.B. Per un giorno ho avuto il problema relativo ad un errore del tipo \"tuple out of index\", ed era relativo al fatto che Image_glob deve essere una lista con più eleemnti, per questo ho inserito .png in più formati. Inizialmente credevo fosse perchè avevo inserito male il path della cartella\n",
    "\n",
    "le righe successive prendono le immagin iuna alla volta (incrementando il contatore ogni volta che next_frame viene chiamata) e la converte in un tensore\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "essendo il codice una versione iniziale che ha ancora dei problemi, se si vuole cambiare la coppia di immagini da analizzare bisogna farlo manualmente. Dalle immagini simulate della luna fornite da Luca, bisogna prenderne una a scelta tra quella relativa alla rotazione al roll,pitch o yaw e lasciarla nella cartella \"MoonImg_Only2\" insiema all'immagine di Nadir che è usata come riferimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8011333f-baf6-468b-b161-3b6b97fe6004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Processing image directory input: ./SuperGluePretrainedNetwork/MoonLu/MoonImg_Only2\n"
     ]
    }
   ],
   "source": [
    "images = VideoStreamer(basedir=\"./SuperGluePretrainedNetwork/MoonLu/MoonImg_Only2\", resize=(640, 480), skip=2, image_glob=[\"*.png\", \"*.PNG\"])\n",
    "\n",
    "image0 = images.next_frame()\n",
    "image1 = images.next_frame()\n",
    "\n",
    "image0_tens = frame2tensor(image0[0], \"cpu\")\n",
    "image1_tens = frame2tensor(image1[0], \"cpu\")\n",
    "\n",
    "#print(type(image0_tens))\n",
    "#print(image1)\n",
    "#print(images.listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e94e4-f966-4bc8-8eb6-4e172b72e3d8",
   "metadata": {},
   "source": [
    "Debug per assicurarsi di leggere tutte le immagini (se skip=1, prendeva tutte le immagini 2 volte ciascuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee70f8f9-2b32-457d-962f-eb792031527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('SuperGluePretrainedNetwork/MoonLu/MoonImg_Only2/Nadir.png'), WindowsPath('SuperGluePretrainedNetwork/MoonLu/MoonImg_Only2/roll_8_deg.png')]\n"
     ]
    }
   ],
   "source": [
    "print(images.listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e758dd-77b6-4f3c-b8d3-b64420eb59eb",
   "metadata": {},
   "source": [
    "Debug per vedere se il formato del tensore fosse corretto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc66bbb-c78d-40b5-9049-878c7d637245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(image0_tens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e18af0-a0d2-49b1-9e73-8667a02ae95b",
   "metadata": {},
   "source": [
    "Richiamo della funzione superpoint dalla cartella models, assegnandogli gli unici pesi disponibili e applicandole per le immagini tramite la funzione forward() che accetta un dizionario contente l'immagine e restituisce un altro dizionario in cui va a listare keypoint, gli score e le descrizioni di ognuno (quest'ultiam fatta in modo da riavere il matching con superglue se le descrizioni combaviano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773e5f79-4446-4a14-a617-d9e98e32b888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antoa\\ImageClassification\\SuperGluePretrainedNetwork/models\\superpoint.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n"
     ]
    }
   ],
   "source": [
    "superpoint = SuperPoint ({'weights_path': 'superpoint_v1.pth'})\n",
    "\n",
    "superimage0 = superpoint.forward({'image': image0_tens})\n",
    "superimage1 = superpoint.forward({'image': image1_tens})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb0ef9-fe73-4fc9-b493-93e0b7cccb56",
   "metadata": {},
   "source": [
    "Debug per vedere il formato dell'output di forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f7cca7-91c2-4d83-94b9-83272c2492e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(superimage0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7a86d-8431-4f34-80fd-431edec7e16a",
   "metadata": {},
   "source": [
    "Applicazione di superglue in modo simile a superpoint.\n",
    "\n",
    "Qui i pesi potevano essere selezionati tra indoor e outdoor, e la funzione forward accettava un dizionario contenente gli output di superpoint e le immagini interessate, ritornando i matches per le due immagini e la relativa affidabilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ef95664-9fc7-4237-b588-42ce4fb14ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antoa\\ImageClassification\\SuperGluePretrainedNetwork/models\\superglue.py:226: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperGlue model (\"indoor\" weights)\n"
     ]
    }
   ],
   "source": [
    "superglue = SuperGlue({'weights_path': 'superglue_outdoor.pth'})\n",
    "\n",
    "data = {\n",
    "    'keypoints0': superimage0[\"keypoints\"][0].unsqueeze(0),\n",
    "    'keypoints1': superimage1[\"keypoints\"][0].unsqueeze(0),\n",
    "    'descriptors0': superimage0[\"descriptors\"][0].unsqueeze(0),\n",
    "    'descriptors1': superimage1[\"descriptors\"][0].unsqueeze(0),\n",
    "    'scores0': superimage0[\"scores\"][0].unsqueeze(0),\n",
    "    'scores1': superimage1[\"scores\"][0].unsqueeze(0),\n",
    "    'image0': torch.from_numpy(image0[0]).unsqueeze(0).unsqueeze(0),\n",
    "    'image1': torch.from_numpy(image1[0]).unsqueeze(0).unsqueeze(0),\n",
    "}\n",
    "\n",
    "# Eseguire SuperGlue per trovare le corrispondenze\n",
    "matches = superglue.forward(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917d6a5-0a6f-4c43-be07-bdb304ccac46",
   "metadata": {},
   "source": [
    "Debug per vedere il formato dell'output di forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9091852-0f84-430a-a4b6-4733b339dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c236e-d82b-454b-844c-4fcada23cbae",
   "metadata": {},
   "source": [
    "Prendiamo i keypoints delle due immagini e gli indici dei matches (a cui rimuoviamo una dimensione con squeeze() in vista del prossimo passaggio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8a05de2-5709-47cd-ae3c-0ce2b5441179",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts0 = superimage0[\"keypoints\"][0].numpy()\n",
    "kpts1 = superimage1[\"keypoints\"][0].numpy()\n",
    "\n",
    "match0 = matches[\"matches0\"].numpy().squeeze()\n",
    "match1 = matches[\"matches1\"].numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c964534-7dc5-4e0b-b8d1-0f4dc55a32ae",
   "metadata": {},
   "source": [
    "Alle prime due righe prendiamo i keypoint dalle due immagini eliminando i punti che non hanno avuto corrispondenze (quelli che avevano match -1)\n",
    "\n",
    "Da riga 4 a 8 ho provato un metodo alternativo in merito alla creazione dei vettori keypoints da usare prima per la creazione della matrice essenziale e poi per la definizioned della posa:\n",
    "assumendo che la variabile \"match1\", ottenuto dalla funzione superglue(), è un vettore che contiene gli indici del vettore kpts0 che corrispondono al vettore kpts1 (oppure -1 se non hanno match), ho provato ad usarlo per ordinare le coordinate nel vettore kpts0 eliminando tutti i  punti assenti di corrispondenze (così coem fatto alla riga 2 per keypoints1).\n",
    "\n",
    "Imponendo la f e c secondo i parametri suggeriti da Luca, ho ricavato la matrice essenziale (facendo ottendere alla funzione stessa la Camera Matrix) e poi la matrice di rotazione e vettore traslazione con recoverPose(). Questi ultimi da capire meglio essendo che non mi è chiaro il sistema di riferimento e come osno definiti entrambi i parametri\n",
    "\n",
    "IMPORTANTE I RISULTATI NELLA CELLA A SEGUIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0484ad4b-7989-4cb1-addb-ab75538ab00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints0 = kpts0[match0 != -1] #metodo 1\n",
    "keypoints1 = kpts1[match1 != -1]\n",
    "\n",
    "keypoints0 = np.empty((0,2), dtype=np.float32)               #metodo 2\n",
    "for i in range(0,len(match1)):                               #metodo 2\n",
    "    if match1[i] != -1:                                      #metodo 2\n",
    "        index = match1[i]                                    #metodo 2\n",
    "        keypoints0 = np.vstack([keypoints0, kpts0[index]])   #metodo 2\n",
    "\n",
    "f = 6.013/(11.7e-3)\n",
    "c = 1024/2\n",
    "E, mask = cv2.findEssentialMat(keypoints0, keypoints1, focal=f, pp=(c, c), method=cv2.RANSAC)\n",
    "_, R, t, mask_pose = cv2.recoverPose(E, keypoints0, keypoints1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1d714-dd31-40db-94bd-03f9067249a9",
   "metadata": {},
   "source": [
    "Stampa separata dei parametri per vederne i risutati\n",
    "\n",
    "I vettori di keypoints0 e keypoints1 risultano avere la stessa lunghezza sia nel caso in cui li ordiniamo, sia nel caso in cui non li ordiniamo (fondamentale per far funzioanre findEssentialMar()), quindi primo check superato.\n",
    "Controllando a capione le coordiante nei vari indici dei vettori, vediamo le corrispondenzeche ci servono, quindi secondo check superato.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "si vede che la matrice essenziale E cambia effettivamente sia nel caso in cui non ordiniamo i vettori di keypoints, sia nel caso in cui li ordiniamo. Quindi questa è una variabile da tenere in considerazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "042e26a6-eae3-4128-8a41-18071832be35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02217193 -0.52527643 -0.36333946]\n",
      " [ 0.49841155 -0.00074285  0.33922004]\n",
      " [ 0.36602448 -0.3053663   0.02949846]]\n"
     ]
    }
   ],
   "source": [
    "# print(matches[\"matches1\"])\n",
    "# print(kpts0[493])\n",
    "# print(kpts1[8])\n",
    "# print(keypoints0[1])\n",
    "# print(keypoints1[1])\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f6a1cc8-2a6a-46a7-bfcc-692628170de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9973968  -0.00750699  0.07171655]\n",
      " [ 0.00823893  0.9999169  -0.00991567]\n",
      " [-0.07163616  0.01048073  0.99737576]]\n",
      "[[-0.42796616]\n",
      " [-0.522523  ]\n",
      " [ 0.73743791]]\n"
     ]
    }
   ],
   "source": [
    "print(R)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c6e03-bc30-4e3a-b9ce-d2f3a4aecc56",
   "metadata": {},
   "source": [
    "Applico la funzione di OpenCV per ottenere vettore rotazione a partire da matrice di rotazione (non ne ho capito molto, ma era una mossa disperata visto che non riesco a risolvere l'intoppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce52913e-2ff0-4f02-92d1-306fb8514148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58483143]\n",
      " [4.11039128]\n",
      " [0.45148705]]\n"
     ]
    }
   ],
   "source": [
    "RotVect, _= cv2.Rodrigues(R)\n",
    "print(np.degrees(RotVect))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f429e-6c24-4783-a66a-f52efa9a9fc0",
   "metadata": {},
   "source": [
    "Applico la funzione prima vista per ricavre gli angoli di eulero in gradi:\n",
    "Semra essere vicino alla risoluzione corretta nel caso di una rotazione in yaw di 45° (manco troppo), ma non neglil altri casi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b73bb781-89a5-457a-96fb-dcdc3ba9cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6020592  4.107968   0.47327725]\n"
     ]
    }
   ],
   "source": [
    "euler_angles = RotMatToEulAng(R)\n",
    "print(euler_angles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
